{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Implement RAG using a website and watsonx.ai \n\nIn the lab **\"Implement RAG Use Cases in watsonx.ai\"** we looked at how to implement a RAG use with our source being from some `.pdf` and `.txt` files. In this example we instead source of content by scraping a given website URL. \n\nThe main difference from the previous example is how data is sourced for our embedding. We'll use open source APIs [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/) and [spacy](https://spacy.io/) to get data from a Web page.\n\nTo get started we'll first verify that you have the necessary dependencies installed to run this notebook.\n\nGo ahead and run the following code cell. **This may take a few seconds to complete.**"}, {"metadata": {}, "cell_type": "code", "source": "# Install dependencies\nimport sys\n!{sys.executable} -m pip install -q chromadb==0.4.22\n!{sys.executable} -m pip install -q ibm_watson_machine_learning==1.0.342\n!{sys.executable} -m pip install -q langchain==0.1.3\n!{sys.executable} -m pip install -q langchain_community==0.0.15\n!{sys.executable} -m pip install -q beautifulsoup4==4.12.3\n!{sys.executable} -m pip install -q spacy==3.7.2\n\n!{sys.executable} -m spacy download en_core_web_md\n", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nspacy 3.7.2 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\nweasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastapi-cli 0.0.3 requires typer>=0.12.3, but you have typer 0.9.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mCollecting en-core-web-md==3.7.1\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from en-core-web-md==3.7.1) (3.7.2)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.3)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\nRequirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.3.4)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.9.4)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (6.1.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.65.0)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.7.1)\nRequirement already satisfied: jinja2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.3)\nRequirement already satisfied: setuptools in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (65.6.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (23.2)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.4.0)\nRequirement already satisfied: numpy>=1.19.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.23.5)\nRequirement already satisfied: language-data>=1.2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.2.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.6.0)\nRequirement already satisfied: pydantic-core==2.18.2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.18.2)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.11.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2024.2.2)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.4)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.0.4)\nRequirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.1.1)\nRequirement already satisfied: marisa-trie>=0.7.7 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.1)\n\u001b[38;5;2m\u2714 Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_md')\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Bring in dependencies\n\nIn this next code cell we'll bring in all the dependencies we'll need for later use.\n\nGo ahead and run the following code cell. **There should be no ouput**"}, {"metadata": {}, "cell_type": "code", "source": "# Bring in dependencies\n# SQLite fix: https://docs.trychroma.com/troubleshooting#sqlite\n# __import__('pysqlite3')\n# import sys\n# sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n\n# WML python SDK\nfrom ibm_watson_machine_learning.foundation_models import Model\nfrom ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\nfrom ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes, DecodingMethods\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport spacy\nimport chromadb\nimport en_core_web_md\n\nnlp = spacy.load(\"en_core_web_md\")\n", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Some important variables\n\nIn this next code cell you'll define some variables that will be used in order to interact with your instance of watsonx.ai.\n\nGo ahead and run the following code cell. **There should be no ouput**"}, {"metadata": {}, "cell_type": "code", "source": "# Update the global variables that will be used for authentication in another function\nwatsonx_project_id = \"f0421f12-fb61-49e5-81a1-b1bc47a0b2c5\"\napi_key = \"NYe1i_1pZtTldIChVvq_Gqi-fsTtETJlI_p85Zaf0aMS\"\ninstance_url = \"https://us-south.ml.cloud.ibm.com\"\n", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Understanding the code\n\nIn this next code cell we'll create some functions that we can use later to interact easier with watsonx.ai. These functions are `get_model`, `create_embedding`, and `create_prompt`: \n\n- `get_model`: Creates a model object that will be used to invoke the LLM\n- `extract_text`: Will pull text from a given website to create embedding from\n- `split_text_into_sentences`: Split the text we extracted into individual sentences and clean them of any unnecessary characters\n- `create_embedding`: Loads text data from a given URL into the in-memory `chromadb` instance\n- `create_prompt`: Generates the prompt that is sent to watsonx.ai API\n   - Notice that in the beginning of the function we query the vector database to retrieve information that\u2019s related to our question (semantic search).\n\nGo ahead and run the following code cell. **There should be no ouput**"}, {"metadata": {}, "cell_type": "code", "source": "def get_model(model_type, max_tokens, min_tokens, decoding, temperature, top_k, top_p):\n    generate_params = {\n        GenParams.MAX_NEW_TOKENS: max_tokens,\n        GenParams.MIN_NEW_TOKENS: min_tokens,\n        GenParams.DECODING_METHOD: decoding,\n        GenParams.TEMPERATURE: temperature,\n        GenParams.TOP_K: top_k,\n        GenParams.TOP_P: top_p,\n    }\n\n    model = Model(\n        model_id=model_type,\n        params=generate_params,\n        credentials={\n            \"apikey\": api_key,\n            \"url\": instance_url\n        },\n        project_id=watsonx_project_id\n    )\n    \n    return model\n\ndef extract_text(url):\n    try:\n        # Send an HTTP GET request to the URL\n        response = requests.get(url)\n\n        # Check if the request was successful\n        if response.status_code == 200:\n            # Parse the HTML content of the page using BeautifulSoup\n            soup = BeautifulSoup(response.text, 'html.parser')\n\n            # Extract contents of <p> elements\n            p_contents = [p.get_text() for p in soup.find_all('p')]\n\n            # Print the contents of <p> elements\n            print(\"\\nContents of <p> elements: \\n\")\n            for content in p_contents:\n                print(content)\n            raw_web_text = \" \".join(p_contents)\n            # remove \\xa0 which is used in html to avoid words break acorss lines.\n            cleaned_text = raw_web_text.replace(\"\\xa0\", \" \")\n            return cleaned_text\n\n        else:\n            print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n\n    except Exception as e:\n        print(f\"An error occurred: {str(e)}\")\n\ndef split_text_into_sentences(text):\n    doc = nlp(text)\n    sentences = [sent.text for sent in doc.sents]\n    cleaned_sentences = [s.strip() for s in sentences]\n    return cleaned_sentences\n\ndef create_embedding(url, collection_name):\n    cleaned_text = extract_text(url)\n    cleaned_sentences = split_text_into_sentences(cleaned_text)\n\n    client = chromadb.Client()\n\n    collection = client.get_or_create_collection(collection_name)\n\n    # Upload text to chroma\n    collection.upsert(\n        documents=cleaned_sentences,\n        metadatas=[{\"source\": str(i)} for i in range(len(cleaned_sentences))],\n        ids=[str(i) for i in range(len(cleaned_sentences))],\n    )\n\n    return collection\n\ndef create_prompt(url, question, collection_name):\n    # Create embeddings for the text file\n    collection = create_embedding(url, collection_name)\n\n    # query relevant information\n    relevant_chunks = collection.query(\n        query_texts=[question],\n        n_results=5,\n    )\n    context = \"\\n\\n\\n\".join(relevant_chunks[\"documents\"][0])\n    # Please note that this is a generic format. You can change this format to be specific to llama\n    prompt = (f\"{context}\\n\\nPlease answer the following question in one sentence using this \"\n              + f\"text. \"\n              + f\"If the question is unanswerable, say \\\"unanswerable\\\". Do not include information that's not relevant to the question.\"\n              + f\"Question: {question}\")\n\n    return prompt\n", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Gluing it together\n\nThe next function, `answer_questions_from_web`, that we create is created to help combine the previous five that we defined. This is the wrapper that we will call when we want to interact with watsonx.ai. \n\nGo ahead and run the following code cell. **There should be no ouput**\n"}, {"metadata": {}, "cell_type": "code", "source": "def answer_questions_from_web(url, question, collection_name):\n    # Specify model parameters\n    model_type = \"meta-llama/llama-2-70b-chat\"\n    max_tokens = 100\n    min_tokens = 50\n    top_k = 50\n    top_p = 1\n    decoding = DecodingMethods.GREEDY\n    temperature = 0.7\n\n    # Get the watsonx model = try both options\n    model = get_model(model_type, max_tokens, min_tokens, decoding, temperature, top_k, top_p)\n\n    # Get the prompt\n    complete_prompt = create_prompt(url, question, collection_name)\n\n    generated_response = model.generate(prompt=complete_prompt)\n    response_text = generated_response['results'][0]['generated_text']\n\n    # Remove trailing white spaces\n    response_text = response_text.strip()\n\n    # print model response\n    print(\"--------------------------------- Generated response -----------------------------------\")\n    print(response_text.strip())\n    print(\"*********************************************************************************************\")\n\n    return response_text\n", "execution_count": 5, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Answering some questions\n\nThe next code cell will use all the previous code we've created so far to source information from the input documents and ask a question about them using watsonx.ai (Notice the return of the `answer_questions_from_web` function). \n\nTo do so we'll pass in a question we want to ask, the web URL we want to reference for said question, and finally the name of the collection where the embeddings exist.\n\nGo ahead and run the next code cell. **You will see output from this cell**"}, {"metadata": {}, "cell_type": "code", "source": "# Try diffrent URLs and questions\nweb_url = \"https://www.ibm.com/products/watsonx-ai\"\nquestion = \"What is Prompt Lab?\"\ncollection_name = \"test_web_RAG\"\n\nanswer_questions_from_web(web_url, question, collection_name)\n", "execution_count": 6, "outputs": [{"output_type": "stream", "text": "\nContents of <p> elements: \n\n \n\n\n  \n  \n      Now available\u2014a next generation enterprise studio for AI builders to train, validate, tune and deploy AI models\n  \n\n\n\n\n    \n\n\nIBM\u00ae watsonx.ai\u2122 AI studio is part of the IBM\u00a0watsonx\u2122 AI and data platform, bringing together new generative AI (gen AI) capabilities powered by foundation models and traditional machine learning (ML) into a powerful studio spanning the AI lifecycle. Tune and guide models with your enterprise data to meet your needs with easy-to-use tools for building and refining performant prompts. With\u00a0watsonx.ai, you can build AI applications in a fraction of the time and with a fraction of the data.\u00a0Watsonx.ai\u00a0offers:\n\nIBM Offers Meta\u2019s Llama 3 Open Models on Watsonx\nChat directly with an LLM in the watsonx.ai 30-day demo. This demo does not include agents or simultaneous chat with multiple models.\nIDC Spotlight: The truth about successful generative AI\nBring your own custom foundation models or work with a suite of curated foundation models offered by IBM. Experiment with open source models through the IBM and Hugging Face partnership to meet the needs of your business.\nUse open-source frameworks and tools for code-based, automated and visual data science capabilities\u2013all in a secure, trusted studio environment.\nLeverage foundation models and generative AI with minimal data, advanced prompt-tuning capabilities, full SDK and API libraries.\nAccelerate the full AI model lifecycle with all the tools and runtimes in one place to train, validate, tune and deploy AI models\u00a0across clouds and on-premises environments.\nIn the Prompt Lab, leverage foundation models to create better AI, faster. Experiment with different prompts for various use cases and tasks. With just a few lines of instruction you can draft job descriptions, classify customer complaints, summarize complex regulatory documents, extract key business information and much more. Quickly tune models for your specific business needs using the latest open source and IBM trained foundation models.\nBuild models either visually or with code, deploy and monitor with end-to-end lifecycle explainability and fairness. Use MLOps to simplify model production from any tool and provide automatic model retraining.\nBusinesses are excited about the prospect of tapping foundation models and ML in one place, with their own data, to accelerate generative AI workloads.\n\u201cWatsonx.ai proved to be very useful. In our research, we liked how it helped our customers (and our development team) to simplify tasks and extend the assistant knowledge without the need to pre-set the whole dialog in advance. It is a next level for us and our customers.\u201d\n\n\u2014 Jindrich Chromy, CEO and co-founder\n\u201cWith this tool, we will need only a quarter of the time compared to before to plan, write and publish an article.\u201d\n\n\u2014 Madeline Scholz\n\u201cIBM\u2019s commitment to cultivating an ecosystem of enterprise AI solutions based on their trustworthy and secure AI platform has proven highly complementary to our go-to-market strategy. We look forward to further cooperation.\u201d\n\n\u2014 Tom Foley, Founder and CEO\n\u201cWe are just at the beginning of this journey. This endeavor with IBM has reinforced our conviction that legal intelligence is on the brink of a transformative era, and Blendow Group is poised to lead this revolution.\u201d\n\n\u2014 Johan Wallquist, Chief Digital Innovation Officer\n\u201cWe are encouraged by the results of this pilot, and we look forward to the next chapter in our Generative AI journey. IBM\u2019s ethical and transparent approach to AI helps us put human values at the center of our transformation initiative.\u201d \u2014 Alceu Meinen, Superintendent of AI and Customer Relations\n\u201cThe IBM team has fully supported our patient-first approach that combines process research and process mining to establish areas of opportunity to improve the patient experience and outcomes. This has helped us to identify where we could pilot process changes and exciting new technologies to have an impact far more quickly than we would have before.\" \u2014 Professor Andy Hardy, Chief Executive Officer\nTake the next step to start operationalizing and scaling generative AI and ML for business.\n1IBM\u2019s statements regarding its plans, directions, and intent are subject to change or withdrawal without notice at IBM\u2019s sole discretion. See Pricing\u00a0for more detail. Unless otherwise specified under Software pricing, all features, capabilities, and potential updates refer exclusively to SaaS. IBM makes no representation that SaaS and software features and capabilities will be the same.\n--------------------------------- Generated response -----------------------------------\nPrompt Lab is a tool that allows users to experiment with different prompts for various use cases and tasks and tune and guide models with their enterprise data to meet their needs with easy-to-use tools for building and refining performant prompts.\n*********************************************************************************************\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 6, "data": {"text/plain": "'Prompt Lab is a tool that allows users to experiment with different prompts for various use cases and tasks and tune and guide models with their enterprise data to meet their needs with easy-to-use tools for building and refining performant prompts.'"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.14", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}