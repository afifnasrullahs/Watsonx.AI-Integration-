{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Implement RAG Use Cases in watsonx.ai\n\nIn this lab you will review and run examples of LLM applications that implement the Retrieval Augmented Generation (RAG) pattern of working with LLMs. We will expand on the concepts that you learned in the previous labs.\n\nRAG (Retrieval-Augmented Generation) is one of the most common use cases in generative AI because it allows us to work with data \"external to the model\", for example, data that was not used for model training. Many use cases require working with proprietary company data, and it's one of the reasons why RAG is frequently used in generative AI applications. RAG also allows us to add some guardrails to generated output and reduce hallucination. RAG can be used with several generative AI use cases, including:\n\n- Question and answer\n- Summarization\n- Content generation\n\n> A \"human interaction\" analogy of RAG is providing a document to a person and asking \nthem to answer question based on the information in the document.\n\nTo get started we'll first verify that you have the necessary dependencies installed to run this notebook.\n\nGo ahead and run the following code cell. **This may take a few seconds to complete.**"}, {"metadata": {}, "cell_type": "code", "source": "# Install dependencies\nimport sys\n!{sys.executable} -m pip install -q chromadb==0.4.22\n!{sys.executable} -m pip install -q ibm_watson_machine_learning==1.0.342\n!{sys.executable} -m pip install -q langchain==0.1.4\n!{sys.executable} -m pip install -q langchain_community==0.0.15\n!{sys.executable} -m pip install -q pypdf==4.0.1\n", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nibm-watsonx-ai 0.2.6 requires ibm-watson-machine-learning>=1.0.349, but you have ibm-watson-machine-learning 1.0.342 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlangchain-text-splitters 0.0.1 requires langchain-core<0.2.0,>=0.1.28, but you have langchain-core 0.1.23 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Bring in dependencies\n\nIn this next code cell we'll bring in all the dependencies we'll need for later use.\n\nGo ahead and run the following code cell. **There should be no ouput**"}, {"metadata": {}, "cell_type": "code", "source": "# Bring in dependencies\n# SQLite fix: https://docs.trychroma.com/troubleshooting#sqlite\n# __import__('pysqlite3')\n# import sys\n# sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n\nimport requests\nimport chromadb\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom chromadb.utils import embedding_functions\n\n# Document loaders\nfrom langchain.document_loaders.pdf import PyPDFLoader\nfrom langchain.document_loaders import TextLoader\nfrom langchain_core.documents import Document\n\n# WML python SDK\nfrom ibm_watson_machine_learning.foundation_models import Model\nfrom ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\nfrom ibm_watson_machine_learning.foundation_models.utils.enums import DecodingMethods\n\nprint(\"Successfully loaded dependencies!\")\n\nFILE_TYPE_TXT = \"txt\"\nFILE_TYPE_PDF = \"pdf\"\n", "execution_count": 2, "outputs": [{"output_type": "stream", "text": "Successfully loaded dependencies!\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Some important variables\n\nIn this next code cell you'll define some variables that will be used in order to interact with your instance of watsonx.ai.\n\nGo ahead and run the following code cell. **There should be no ouput**"}, {"metadata": {}, "cell_type": "code", "source": "# Update the global variables that will be used for authentication in another function\nwatsonx_project_id = \"f0421f12-fb61-49e5-81a1-b1bc47a0b2c5\"\napi_key = \"NYe1i_1pZtTldIChVvq_Gqi-fsTtETJlI_p85Zaf0aMS\"\nurl = \"https://us-south.ml.cloud.ibm.com\"\n", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Understanding the code\n\nIn this next code cell we'll create some functions that we can use later to interact easier with watsonx.ai. These functions are `get_model`, `create_embedding`, and `create_prompt`: \n\n- `get_model`: Creates a model object that will be used to invoke the LLM\n- `create_embedding`: Loads text data from given file path into the in-memory `chromadb` instance\n- `create_prompt`: Generates the prompt that is sent to watsonx.ai API\n   - Notice that in the beginning of the function we query the vector database to retrieve information that\u2019s related to our question (semantic search). Search results are appended to the prompt, and the prompt instruction is \"to give an answer using the provided text\".\n\nGo ahead and run the following code cell. **There should be no ouput**"}, {"metadata": {}, "cell_type": "code", "source": "prompt_template = \"\"\"\nAnswer the following question using the context provided. \nIf there is no good answer, say \"unanswerable\".\n\nContext: %s\n\nQuestion: %s\n\"\"\"\n\n# Creates a model object that will be used to invoke the LLM\ndef get_model(model_type,max_tokens,min_tokens,decoding,temperature):\n\n    generate_params = {\n        GenParams.MAX_NEW_TOKENS: max_tokens,\n        GenParams.MIN_NEW_TOKENS: min_tokens,\n        GenParams.DECODING_METHOD: decoding,\n        GenParams.TEMPERATURE: temperature\n    }\n\n    model = Model(\n        model_id=model_type,\n        params=generate_params,\n        credentials={\n            \"apikey\": api_key,\n            \"url\": url\n        },\n        project_id=watsonx_project_id\n        )\n\n    return model\n\n# Loads text data from given file path into the chromadb instance\ndef create_embedding(file_path,file_type,collection_name):\n    documents = []\n\n    if file_type == FILE_TYPE_TXT:\n        if file_path.startswith('http'):\n            r = requests.get(file_path)\n            metadata = {\"source\": file_path}\n            raw_text = r.text.encode('utf-8').strip()\n            documents = [Document(page_content=raw_text, metadata=metadata)]\n        else:\n            loader = TextLoader(file_path,encoding=\"1252\")\n            documents = loader.load()        \n    elif file_type == FILE_TYPE_PDF:\n        loader = PyPDFLoader(file_path)\n        documents = loader.load()\n\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n    texts = text_splitter.split_documents(documents)\n  \n    print(type(texts))\n    print(len(texts))\n\n    # Load chunks into chromadb\n    client = chromadb.Client()\n    collection = client.get_or_create_collection(name=collection_name,embedding_function=embedding_functions.DefaultEmbeddingFunction())\n    collection.upsert(\n        documents=[doc.page_content for doc in texts],\n        ids=[str(i) for i in range(len(texts))],  # unique for each doc\n    )\n\n    return collection\n\n# Generates the prompt that is sent to watsonx.ai API\ndef create_prompt(file_path, file_type, question, collection_name):\n    # Create embeddings for the text file\n    collection = create_embedding(file_path,file_type,collection_name)\n\n    # Query relevant information\n    relevant_chunks = collection.query(\n        query_texts=[question],\n        n_results=3,\n    )\n\n    context = \"\\n\\n\\n\".join(relevant_chunks[\"documents\"][0])\n    prompt = prompt_template % ( context, question )\n    return prompt\n", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Gluing it together\n\nThe next function, `answer_questions_from_doc`, that we create is created to help combine the previous three that we define. This is the wrapper that we will call when we want to interact with watsonx.ai. \n\nGo ahead and run the following code cell. **There should be no ouput**"}, {"metadata": {}, "cell_type": "code", "source": "def answer_questions_from_doc(file_path,file_type,question,collection_name):\n\n    # Specify model parameters\n    model_type = \"meta-llama/llama-2-70b-chat\"\n    max_tokens = 300\n    min_tokens = 100\n    decoding = DecodingMethods.GREEDY\n    temperature = 0.7\n\n    # Get the watsonx model\n    model = get_model(model_type, max_tokens, min_tokens, decoding, temperature)\n\n    # Get the prompt\n    complete_prompt = create_prompt(file_path, file_type, question, collection_name)\n\n    generated_response = model.generate(prompt=complete_prompt)\n    response_text = generated_response['results'][0]['generated_text']\n\n    # print model response\n    print(\"--------------------------------- Generated response -----------------------------------\")\n    print(response_text.strip(\"\\n\"))\n    print(\"*********************************************************************************************\")\n\n    return response_text\n", "execution_count": 5, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Answering some questions\n\nThe next code cell will use all the previous code we've created so far to source information from the input documents and ask a question about them using watsonx.ai (Notice the return of the `answer_questions_from_doc`). \n\nTo do so we'll pass in a question we want to ask, the file we want to reference for said question, and finally the name of the collection where the embeddings of the file exist.\n\nGo ahead and run the next code cell. **You will see output from this cell**"}, {"metadata": {}, "cell_type": "code", "source": "# Test answering questions based on the provided .txt file\nquestion = \"What did the president say about corporate tax?\"\nfile_path = \"https://raw.githubusercontent.com/CloudPak-Outcomes/Outcomes-Projects/main/L4assets/watsonx.ai-Assets/Documents/state_of_the_union.txt\"\ncollection_name = \"state_of_the_union_remote\"\nanswer_questions_from_doc(file_path,FILE_TYPE_TXT, question, collection_name)\n\n# Test answering questions based on the provided .pdf file\nquestion = \"How can you build a Generative AI model?\"\nfile_path = \"https://raw.githubusercontent.com/CloudPak-Outcomes/Outcomes-Projects/main/L4assets/watsonx.ai-Assets/Documents/Generative_AI_Overview.pdf\"\ncollection_name = \"generative_ai_doc\"\nanswer_questions_from_doc(file_path, FILE_TYPE_PDF, question, collection_name)\n", "execution_count": 6, "outputs": [{"output_type": "stream", "text": "<class 'list'>\n92\n", "name": "stdout"}, {"output_type": "stream", "text": "/home/wsuser/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 79.3M/79.3M [00:01<00:00, 54.8MiB/s]\n", "name": "stderr"}, {"output_type": "stream", "text": "--------------------------------- Generated response -----------------------------------\nAnswer: According to the context, the president proposed a 15% minimum tax rate for corporations so that they can't avoid paying taxes by shipping jobs and factories overseas. The president also mentioned that more than 130 countries agreed on a global minimum tax rate to prevent companies from dodging taxes. The president emphasized that this plan will lower costs and ensure that corporations and the wealthiest Americans start paying their fair share of taxes.\n*********************************************************************************************\n<class 'list'>\n33\n--------------------------------- Generated response -----------------------------------\nAnswer: Building a generative AI model requires a large amount of sample text, such as a broad swath of the internet, and the right algorithms, such as those used in ChatGPT. Additionally, it requires a significant amount of computational power and data to train the model. The model can then be trained on this data to generate new content, such as text, images, audio, and simulations. Recent breakthroughs in the field have made it possible to create generative AI models that can create realistic and accurate content.\n*********************************************************************************************\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 6, "data": {"text/plain": "'\\nAnswer: Building a generative AI model requires a large amount of sample text, such as a broad swath of the internet, and the right algorithms, such as those used in ChatGPT. Additionally, it requires a significant amount of computational power and data to train the model. The model can then be trained on this data to generate new content, such as text, images, audio, and simulations. Recent breakthroughs in the field have made it possible to create generative AI models that can create realistic and accurate content.'"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.14", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}