{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Retrieval Augmented Generation using watsonx.ai and Vector Database\n\nIn this notebook, we'll demonstrate how to utilize a Vector Database to retrieve relevant passages based on a user query. We'll then append these passages as context to the prompt that will be passed to the LLM in watsonx.ai for generation."}, {"metadata": {}, "cell_type": "markdown", "source": "## Introduction\n\nRetrieval Augmented Generation (RAG) is a powerful technique that combines the strengths of pre-trained large language models (LLM) and information retrieval systems to generate responses based on a given context. In this notebook, we will be using a Vector Database and watsonx.ai foundation models to implement a RAG use-case.\n\nA vector database (or store), when applied to text data, is a specialized database that efficiently stores embeddings, representing pieces of text, for efficient  queries. It enables quick similarity searches, allowing you to pinpoint texts that are _'similar'_ based on their vectorized representations. For our purposes, we will use Chroma, an open-source embedding database.\n\nInstead of using Watson Discovery to pass back the relevant passages, we are using a vector database called Chroma. Chroma is mainly used to parse through the PDFs, store the content, and then query from that collection. The code in the notebook below demonstrates the implementation of this approach."}, {"metadata": {}, "cell_type": "markdown", "source": "### Pre-requisites\n\nThis lab should take about 45 minutes.\n\nBefore we begin lets start off by ensuring we have completed some pre-requisites; ensure you gave the following\n\n- IBM Cloud API key \n- Project ID associated with your watsonx instance\n\nYou can use the following support links if you need any help with the pre-requisites above\n\n- [Creating IBM Cloud API Key](https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui#create_user_key)\n- [Finding watsonx Project ID](https://www.ibm.com/docs/en/watsonx-as-a-service?topic=library-project-id)"}, {"metadata": {}, "cell_type": "markdown", "source": "### Setting up"}, {"metadata": {}, "cell_type": "markdown", "source": "#### Importing Required Libraries\n\nBefore we get started looking at some code, we will need to install some dependencies for our notebook; the following notebook cell will do just that."}, {"metadata": {}, "cell_type": "code", "source": "# Download dependencies\n\nimport sys\n!{sys.executable} -m pip install -q langchain\n!{sys.executable} -m pip install -q chromadb\n!{sys.executable} -m pip install -q pypdf\n\n!{sys.executable} -m pip install -q ibm_cloud_sdk_core\n!{sys.executable} -m pip install -q ibm_watson_machine_learning\n", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": ""}, {"metadata": {}, "cell_type": "code", "source": "# Import necessary modules and packages\n\nfrom ibm_cloud_sdk_core import IAMTokenManager\nfrom ibm_watson_machine_learning.foundation_models import Model\n\nimport langchain.embeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.document_loaders.pdf import PyPDFLoader\n\nfrom sentence_transformers import SentenceTransformer\nfrom typing import Optional, Iterable, List\n", "execution_count": 4, "outputs": [{"output_type": "error", "ename": "ImportError", "evalue": "cannot import name 'PyPDFLoader' from 'langchain.document_loaders.pdf' (/opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages/langchain/document_loaders/pdf.py)", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)", "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpdf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyPDFLoader\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional, Iterable, List\n", "\u001b[0;31mImportError\u001b[0m: cannot import name 'PyPDFLoader' from 'langchain.document_loaders.pdf' (/opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages/langchain/document_loaders/pdf.py)"]}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Embedding & Vector Database"}, {"metadata": {}, "cell_type": "markdown", "source": "#### Creating Embeddings Class\n\nTo start off we will create a custom class, **MiniLML6V2EmbeddingFunctionLangchain**, and define some functions which are designed to generate embeddings using the `MiniLM-L6-v2` model from the `sentence_transformers` library. This class will serve as our embedding function where text we want to store in vector format will be processed before being stored within a vector database. As a quick reminder, embeddings are used to create a vector representation of the text data and capture the semantic meaning."}, {"metadata": {}, "cell_type": "code", "source": "class MiniLML6V2EmbeddingFunctionLangchain(langchain.embeddings.openai.Embeddings):\n    MODEL = SentenceTransformer('all-MiniLM-L6-v2')\n    def embed_documents(self, texts):\n        return MiniLML6V2EmbeddingFunctionLangchain.MODEL.encode(texts).tolist()\n    \n    def embed_query():\n        super().embed_query()\n \nprint('done')\n", "execution_count": 5, "outputs": [{"output_type": "error", "ename": "AttributeError", "evalue": "module langchain.embeddings has no attribute openai", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)", "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMiniLML6V2EmbeddingFunctionLangchain\u001b[39;00m(\u001b[43mlangchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopenai\u001b[49m\u001b[38;5;241m.\u001b[39mEmbeddings):\n\u001b[1;32m      2\u001b[0m     MODEL \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts):\n", "File \u001b[0;32m/opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages/langchain/embeddings/__init__.py:167\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Look up attributes dynamically.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_import_attribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n", "File \u001b[0;32m/opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages/langchain/_api/module_import.py:168\u001b[0m, in \u001b[0;36mcreate_importer.<locals>.import_by_name\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    165\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfallback_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n", "\u001b[0;31mAttributeError\u001b[0m: module langchain.embeddings has no attribute openai"]}]}, {"metadata": {}, "cell_type": "markdown", "source": "#### Creating A VectorDB Class\n\nWe will also create a custom class, **ChromaWithUpsert**, which is an abstraction using `Chroma` class from the `Chroma` vectorstore class in the langchain module. Using this class we introduce the ability to _upsert_ texts within the vector database _(either adding or updating)_. The _upsert_texts_ method from our class takes in the text content, their metadata _(i.e. source document)_, and their ids _(if provided)_, and generates the embeddings using the class defined earlier before adding the newly created vector in to the `Chroma` vector database."}, {"metadata": {}, "cell_type": "code", "source": "class ChromaWithUpsert(Chroma):\n    def upsert_texts(\n        self,\n        texts: Iterable[str],\n        metadatas: Optional[List[dict]] = None,\n        ids: Optional[List[str]] = None,\n    ) -> List[str]:\n        \"\"\"Run more texts through the embeddings and add to the vectorstore.\n        Args:\n            texts (Iterable[str]): Texts to add to the vectorstore.\n            metadatas (Optional[List[dict]], optional): Optional list of metadatas.\n            ids (Optional[List[str]], optional): Optional list of IDs.\n        Returns:\n            List[str]: List of IDs of the added texts.\n        \"\"\"\n        \n        if ids is None:\n            import uuid\n            ids = [str(uuid.uuid1()) for _ in texts]\n        embeddings = None\n\n        if self._embedding_function is not None:\n            embeddings = self._embedding_function.embed_documents(texts = list(texts))\n\n        self._collection.upsert(\n            metadatas=metadatas, embeddings=embeddings, documents=texts, ids=ids\n        )\n        return ids\n    \n    def query(self, query_texts:str, n_results:int=5, include: Optional[List[str]]=None):\n        return self._collection.query(\n            query_texts=query_texts,\n            n_results=n_results,\n            include=include\n        )\n\nprint('done')\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Loading and Splitting PDF Text Content\n\nIn the following cell we are loading PDF documents using the **PyPDFLoader** class and storing it in the data variable. Our PDF is being loaded from a URL and will be used to represent our existing knowledge base.\n\nThe loaded data is then split into smaller chunks using the **RecursiveCharacterTextSplitter** class, which allows us to split long text on predefined characters that are considered potential division points . The size of the chunks and the overlap between them is defined by `CHUNK_SIZE` and `CHUNK_OVERLAP` variables."}, {"metadata": {}, "cell_type": "code", "source": "loader = PyPDFLoader(\"https://www.captiveaire.com/manuals/exhaustfans/exhaust-oim.pdf\")\ndata = loader.load()\n\nCHUNK_SIZE = 1000\nCHUNK_OVERLAP = 10\n\ntext_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=CHUNK_SIZE,\n            chunk_overlap=CHUNK_OVERLAP\n        )\ntexts = text_splitter.split_documents(data)\nprint('done')\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Saving texts to a VectorDB\n\n\nOnce our loaded text is split we can now can create an instance of our vector database using the `ChromaWithUpsert` class with our custom embedding function and a collection name. Once defined, using the `upsert_texts` method, we add the split texts and their metadata to the vector database. "}, {"metadata": {}, "cell_type": "code", "source": "vector_store = ChromaWithUpsert(\n    collection_name=f\"store_minilm6v2\",\n    embedding_function=MiniLML6V2EmbeddingFunctionLangchain(),\n)\n\nvector_store.upsert_texts(\n        texts=[doc.page_content for doc in texts],\n        metadatas=[doc.metadata for doc in texts]\n)\nprint('done')\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Set up the Language Learning Model (LLM)\n\nIn this cell, we are setting up the parameters for the Language Learning Model (LLM). This includes our IBM Cloud API Key and watsonx Project ID in order to make use of `watsonx.ai` foundation models. Default tuning parameters (gen) are provided, but can be adjusted as needed; aAfter setting up these parameters, we will use them to initialize our LLM (watsonx.ai) in the next cell.\n\nIf you want to learn more about watsonx.ai foundation models tuning paremeter, you can visit the watsonx.ai foundation [documentation link here](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-parameters.html?context=wx&audience=wdp)\n"}, {"metadata": {}, "cell_type": "code", "source": "# Your IBM Cloud API key\napi_key = \"INSERT YOUR API KEY HERE\"\n\n# Project ID of your watsonx project\nwatsonx_project_id = \"INSERT YOUR watsonx PROJECT ID HERE\"\n\n# LLM that we want to use with watsonx.ai\nmodel_id= \"google/flan-ul2\"\n\nendpoint= \"https://us-south.ml.cloud.ibm.com\"\n\naccess_token = ''\n\ntry:\n  access_token = IAMTokenManager(\n    apikey = api_key,\n    url = \"https://iam.cloud.ibm.com/identity/token\"\n  ).get_token()\nexcept:\n  print('Issue obtaining access token. Check variables?') \n\ncredentials = { \n    \"url\"    : endpoint, \n    \"token\" : access_token\n}\n\n# watsonx.ai tuning parameters\ngen_params = {\n    \"DECODING_METHOD\" : \"greedy\",\n    \"MAX_NEW_TOKENS\" : 300,\n    \"MIN_NEW_TOKENS\" : 1,\n    \"TEMPERATURE\" : 0.7,\n    \"TOP_K\" : 50,\n    \"TOP_P\" : 0.15,\n    \"REPETITION_PENALTY\" : 2.0\n}\n\nmodel = Model( model_id, credentials, gen_params, watsonx_project_id )\nprint('done')\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Combining watsonx.ai LLM and VectorDB"}, {"metadata": {}, "cell_type": "markdown", "source": "#### Constrcut Query & Identify the relevant texts in the documents\n\nIn this cell, we are constructing the query prompt for the Language Learning Model (LLM). The query is the question that we want to ask our foundation model. This question will be used to retrieve the relevant texts from the documents in our vector database.\n\nWe specify the number of text passages we want returned from our vector database using the `search_k` variable _(in this case, we use 3)_. If you find that you are not getting very good answers, you can increase the `search_k` variable, in order to increase the amount of context (number of matching passages) provided to \n\nWe will store the best relevant text passage along with its metadata and distances, which identify the source and page number and join them all into our `context` variable."}, {"metadata": {}, "cell_type": "code", "source": "question = 'For power roof ventilators should dampers be installed when an exhauster is used?'\n\nsearch_k = 13\ndocs = []\ndocs = vector_store.query(\n            query_texts=[question],\n            n_results=search_k,\n            include=[\"documents\",\"metadatas\", \"distances\"]\n        )\n\ncontext = \" \".join(docs[\"documents\"][0])\nprint('done')\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Construct the Prompt & Query watsonx.ai\n\nNow, we combine the query and the context we received from the vector database into a prompt. We created a custom function to take in both the query and context.\n\nWe will then query our foundation model from watsonx.ai that we created earlier; given that we _upserted_ the documents with the metadata of the source and documents, we can identify which document and where in that document that we are using text context from in order to answer the question."}, {"metadata": {}, "cell_type": "code", "source": "#######################################################################################\nprompt_template = \"\"\"\nAnswer the following question using the context provided. \nIf there is no good answer, say \"I don't know\".\n\nContext: %s\n\nQuestion: %s\n\"\"\"\n\n#######################################################################################\ndef augment( template_in, context_in, query_in ):\n    return template_in % ( context_in,  query_in )\n\n#######################################################################################\nimport json\n\ndef generate( model_in, augmented_prompt_in ):\n    \n    generated_response = model_in.generate( augmented_prompt_in )\n \n    if ( \"results\" in generated_response ) \\\n       and ( len( generated_response[\"results\"] ) > 0 ) \\\n       and ( \"generated_text\" in generated_response[\"results\"][0] ):\n        return generated_response[\"results\"][0][\"generated_text\"]\n    else:\n        print( \"The model failed to generate an answer\" )\n        print( \"\\nDebug info:\\n\" + json.dumps( generated_response, indent=3 ) )\n        return \"\"\n\n########################################################################################\nimport re\n\naugmented_prompt = augment( prompt_template, context, question)\noutput = generate( model, augmented_prompt )\nif not re.match( r\"\\S+\", output ):\n    print( \"The model failed to generate an answer\")\nprint( \"\\nAnswer:\\t\" + output )\n\nsource_file = docs['metadatas'][0][0]['source']\npage = docs['metadatas'][0][0]['page']\n\nprint('\\nSource\\t', source_file)\nprint('Page\\t',page)\nprint('done')\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Congratulations you just completed a RAG implementaion using VectorDB. Feel free to re-run the prompt by asking other questions or change the PDF used to provide watsonx.ai with a different context. "}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.14", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}